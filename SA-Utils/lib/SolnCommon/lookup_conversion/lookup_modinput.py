'''
Copyright (C) 2005 - 2013 Splunk Inc. All Rights Reserved.
'''
import abc
import copy
import os
# import pprint  # for debugging
import threading
import time
import splunk
import sys
from splunk.appserver.mrsparkle.lib.util import make_splunkhome_path
sys.path.append(make_splunkhome_path(["etc", "apps", "SA-Utils", "lib"]))
from SolnCommon.lookups import get_lookup_table_file_update_times
from SolnCommon.modinput import logger
from SolnCommon.modinput import ModularInput
from SolnCommon.pooling import should_execute
# These imports are local to the app that contains this modular input script.
from lookup_conversion import conversion_handlers
from lookup_conversion import conversion_specifications
from lookup_conversion import merge_handlers
from lookup_conversion import output_handlers
from lookup_conversion import output_specifications


class LookupModularInput(ModularInput):

    __metaclass__ = abc.ABCMeta

    def __init__(self, scheme_args, args):

        super(LookupModularInput, self).__init__(scheme_args, args)
    
    @abc.abstractmethod
    def run_threads(self):
        pass
    
    @property
    def app(self):
        return self._app
    
    @property
    def name(self):
        return self._name

    @property
    def owner(self):
        return self._owner
    
    def collect_stanzas(self, stanzas, *args, **kwargs):
        '''Override this method in the cases where one modular input needs to
        read the stanzas from another modular input.'''
        return stanzas
    
    def collect_files(self):
        '''Collect the source file information for each stanzas, grouped by target.
        Stanzas may be stanzas as passed in from the modular input, or as obtained
        via a REST call. Exception checking is used to distinguish between
        dict-like objects and "true" stanzas.
        
        If modinput_name refers to another modular input, that input's name 
        instead of the current input's name will be used to obtain the files.
        This allows one modular input to read the output generated by another
        modular input in a producer-consumer relationship. Note that this only
        applies to checkpointed files.
        
        A stanza may be eliminated from contention in two ways:
        - the stanza is disabled
        - the stanza's target does not match a valid target of the modular input
          that subclassed this modular input
          
        Returns:
        
        A dictionary of { target -> [(stanza_name, path, last_updated), ... ] }
         
        Each path will exist. The caller should handle testing for readability.
        '''
        lookup_src_to_tgt_map = {}
        checkpoint_src_to_tgt_map = {}
        
        for stanza in self._stanzas:
            try:
                disabled = stanza.get('disabled', False)
                name = stanza.get('name')
                protocol, url = stanza.get('url').split('://')
                target = stanza.get('target')
                if not disabled and target:
                    if protocol == 'lookup':
                        lookup_src_to_tgt_map[url] = target
                    elif protocol in ['http', 'https']:
                        # Assume the file has been checkpointed following a download.
                        # "name" instead of "url" is used in this case since the
                        # checkpoint file is based on the stanza name.
                        checkpoint_src_to_tgt_map[name] = target
                    else:
                        logger.exception('Invalid stanza (exception handled; traceback follows): {}'.format(stanza))
                else:
                    # Stanza is disabled or has no valid target.
                    pass
            except (ValueError, AttributeError):
                logger.exception('Invalid stanza (exception handled; traceback follows): {}'.format(stanza))

        files_by_target = {}
        lookup_update_times = []
        checkpoint_update_times = []

        try:
            lookup_update_times = get_lookup_table_file_update_times(lookup_src_to_tgt_map.keys(), session_key=self._input_config.session_key)
        except splunk.RESTException as e:
            logger.exception('status="REST exception when querying lookup table files" exc={}'.format(e))

        try:
            # If an alternate modular input name has been specified in the modular
            # input's __init__ method, use it here to retrieve checkpoint file
            # update times.
            if hasattr(self, '_alt_modinput_name'):
                checkpoint_update_times = self.get_checkpoint_update_times(checkpoint_src_to_tgt_map.keys(), self._alt_modinput_name)
            else:
                checkpoint_update_times = self.get_checkpoint_update_times(checkpoint_src_to_tgt_map.keys(), None)
        except IOError as e:
            logger.exception('status="I/O exception when reading checkpoint files" exc={}'.format(e))

        # Add the lookup file paths and update times to the output.
        for name, path, last_updated in lookup_update_times:
            category = lookup_src_to_tgt_map.get(name).lower()
            if path and last_updated:
                curr = files_by_target.setdefault(category, [])
                curr.append((name, path, last_updated))
                logger.info('status="Lookup table file found" name={} category={} path={} last_updated={}'.format(name, category, path, last_updated))
            else:
                logger.error('status="Lookup table file error" err="unknown path or update time" name={} category={}'.format(name, category))

        # Add the checkpoint file paths and update times to the output.
        for name, path, last_updated in checkpoint_update_times:
            category = checkpoint_src_to_tgt_map.get(name).lower()
            if path and last_updated:
                curr = files_by_target.setdefault(category, [])
                curr.append((name, path, last_updated))
                logger.info('status="Checkpoint file found" name={} category={} path={} last_updated={}'.format(name, category, path, last_updated))
            else:
                logger.error('status="Checkpoint file error" err="unknown path or update time" name={} category={}'.format(name, category))

        return files_by_target
    
    def get_handlers(self, merge_class, prefix=''):
        '''Obtain the various handler classes dynamically.
        This is used in case where the same modular input runs multiple merge 
        threads. For instance, identity_managet.py runs two threads: "asset"
        and "identity". Thus the classes it instantiates are named according to:

           modinput_name     + merge_class   + handler_name
        
           'IdentityManager' + 'Asset'       + 'OutputHandler'
           'IdentityManager' + 'Identity'    + 'OutputHandler'
           etc.

        In cases where only one merge_class is used, the merge class may be 
        used in isolation and name should be set to an empty string:

           <empty>           + 'Threatlist'  +  'OutputHandler'

        '''

        m = merge_class.capitalize()
        try:
            # The unresolved imports in the next 5 lines are OK. These imports
            # occur in the class that instantiates this class, since the imports
            # are local to the <app>/bin/lookup_expansion directory.
            merge_handler_cls = getattr(merge_handlers, '{0}{1}MergeHandler'.format(prefix, m))
            conversion_spec_cls = getattr(conversion_specifications, '{0}{1}ConversionSpec'.format(prefix, m))
            conversion_handler_cls = getattr(conversion_handlers, '{0}{1}ConversionHandler'.format(prefix, m))
            output_spec_cls = getattr(output_specifications, '{0}{1}OutputSpec'.format(prefix, m))
            output_handler_cls = getattr(output_handlers, '{0}{1}OutputHandler'.format(prefix, m))
        except AttributeError:
            raise
        
        return (merge_handler_cls,
                conversion_spec_cls,
                conversion_handler_cls,
                output_spec_cls,
                output_handler_cls)

    def merge_task(self, *args, **kwargs):
        '''Merge asset lists into a single Splunk-formatted lookup table.'''

        # kwargs is expected to contain a dictionary of:
        # <merge_class> -> [(stanza_name, path, last_updated), (stanza_name, path, last_updated)...]
        
        merge_class = threading.current_thread().getName()
        logger.info('status="merge task starting" merge_class="{}"'.format(merge_class))
        event = args[0]
        files = kwargs.get(merge_class, None)
        
        if files:

            try:
                # Initialize all the handlers: merge -> convert -> output
                merge_handler_cls, conversion_spec_cls, conversion_handler_cls, output_spec_cls, output_handler_cls = self.get_handlers(merge_class, prefix=self.name)
                merge_handler = merge_handler_cls(files=files, stanzas=self._stanzas)
                # Sometimes a spec needs to collect REST information; pass in the arguments here.
                conversion_spec = conversion_spec_cls(namespace=self.app, owner=self.owner, session_key=self._input_config.session_key)
                conversion_handler = conversion_handler_cls(conversion_spec, self._input_config.session_key)
                output_spec = output_spec_cls(self.app, self.owner)
                output_handler = output_handler_cls(conversion_spec, output_spec, self._input_config.session_key)

            except Exception as e:
                # Merge cannot continue.
                logger.exception('status="error initializing merge process" merge_class="{}" exc="{}"'.format(merge_class, e))
                sys.exit(1)

            # Convert input to Splunk lookup table format.
            output_handler.setup_writers()
            conversion_handler.setup_streamed_output()
            
            # Counters for intermediate output.
            success_count = 0
            failed_count = 0
            power = 1
            limit = 1
            
            try:
                merged_data = merge_handler.collect()
                
                # record is a dictionary representing the row of the input.
                for record in conversion_handler.process(merged_data):
                    try:
                        success_count += 1
                        success = output_handler.process_streaming_record(record)

                        # Provide intermediate output.
                        # Note: because we don't stream the input rows, counts 
                        # here may be inflated due to subnet expansion.
                        if success_count % limit == 0:
                            logger.info('status="processed %s rows"', success_count)
                            power += 1
                            limit = pow(2, power)

                    except:
                        failed_count += 1
                        logger.exception('status="Error processing record" count="%s" record="%s"', success_count, record)
                
            except (ValueError, IndexError, IOError) as e:
                logger.exception('status="Exception when reading input files"')
            except Exception as e:
                logger.exception('status="Unknown exception when reading input files"')
                sys.exit(1)

            ancillary_data = conversion_handler.finalize_streaming()
            output_handler.write_ancillary_data(ancillary_data)
            output_handler.close_writers()
            
            # Set the event so caller can know whether the merge succeeded.
            event.set()
            logger.info('status="updated lookups" merge_class="%s" lines=%s errors=%s', merge_class, success_count, failed_count)

    def streaming_merge_task(self, *args, **kwargs):
        '''Merge asset lists into a single Splunk-formatted lookup table.'''

        # kwargs is expected to contain a dictionary of:
        # <merge_class> -> [(stanza_name, path, last_updated), (stanza_name, path, last_updated)...]
        
        merge_class = threading.current_thread().getName()
        logger.info('status="merge task starting" merge_class="{}"'.format(merge_class))
        event = args[0]
        files = kwargs.get(merge_class, None)
        
        if files:

            try:
                # Initialize all the handlers: merge -> convert -> output
                merge_handler_cls, conversion_spec_cls, conversion_handler_cls, output_spec_cls, output_handler_cls = self.get_handlers(merge_class, prefix=self.name)
                merge_handler = merge_handler_cls(files=files, stanzas=self._stanzas)
                # Sometimes a spec needs to collect REST information; pass in the arguments here.
                conversion_spec = conversion_spec_cls(namespace=self.app, owner=self.owner, session_key=self._input_config.session_key)
                conversion_handler = conversion_handler_cls(conversion_spec, self._input_config.session_key)
                output_spec = output_spec_cls(self.app, self.owner)
                output_handler = output_handler_cls(conversion_spec, output_spec, self._input_config.session_key)

            except Exception as e:
                # Merge cannot continue.
                logger.exception('status="error initializing merge process" merge_class="{}" exc="{}"'.format(merge_class, e))
                sys.exit(1)

            output_handler.setup_writers()
            conversion_handler.setup_streamed_output()
            
            # Counters for intermediate output.
            success_count = 0
            failed_count = 0
            power = 1
            limit = 1
            try:
                for record in merge_handler.collect():
                    try:
                        # record is a dictionary representing the row of the input.
                        results = conversion_handler.process_streaming_record(record)
                        success_count += 1
                        for result in results:
                            # result is a tuple of (key, value, records)
                            success = output_handler.process_streaming_record(result)

                        # Provide intermediate output.
                        if success_count % limit == 0:
                            logger.info('status="processed %s rows"', success_count)
                            power += 1
                            limit = pow(2, power)

                    except:
                        failed_count += 1
                        logger.exception('status="Error processing record" count="%s" record="%s"', success_count, record)
                        
                ancillary_data = conversion_handler.finalize_streaming()
                output_handler.write_ancillary_data(ancillary_data)
                output_handler.close_writers()

            except (ValueError, IndexError, IOError) as e:
                logger.exception('status="Exception when reading input files" exc={}'.format(e))
            except Exception as e:
                logger.exception('status="Unknown exception when reading input files" exc={}'.format(e))
                sys.exit(1)

            # Set the event so caller can know whether the merge succeeded.
            event.set()
            logger.info('status="updated lookups" merge_class="%s" lines=%s errors=%s', merge_class, success_count, failed_count)

    def detect_changed_stanzas(self, files_by_category):
        '''Compare the current list of files to the previously checkpointed set.
        
        Arguments:
            files_by_category -- A dictionary of category --> [list of tuples (path, last_updated)]
        
        Returns:
            A set of strings containing zero or more strings representing lookup
            categories. Currently the following are used:

                "asset", "identity", "threatlist"
        '''
        PREVIOUS_STANZAS = 'previous_stanzas'
        merge_required = set()

        if self.checkpoint_data_exists(PREVIOUS_STANZAS):
            previous = self.get_checkpoint_data(PREVIOUS_STANZAS)
        else:
            previous = None
            logger.info('status="No checkpoint file; forcing initial merge."')

        if previous:
            # Compare previous to current, if it could be found.
            for key, value in files_by_category.iteritems():
                curr = {path for stanza_name, path, last_updated in value}
                if key in previous:
                    # The if statement gating this set comprehension is required to
                    # avoid an error if for some reason the checkpoint data did 
                    # not contain the values for one of the categories.
                    prev = {path for stanza_name, path, last_updated in previous.get(key)}
                else:
                    prev = set()
                
                if prev != curr:
                    merge_required.add(key)
                    # Log the changes
                    for filename in prev - curr: 
                        logger.info('status="forcing merge due to stanza being deleted or disabled" merge_class="{}" filename="{}"'.format(key, filename))
                    for filename in curr - prev: 
                        logger.info('status="forcing merge due to stanza being added or enabled" merge_class="{}" filename="{}"'.format(key, filename))
                else:
                    # No changes; last_updated timestamps will be used to determine
                    # merge status.
                    pass
        else:
            # No information about previous stanzas could be retrieved;
            # assume that a merge is required if we cannot determine status.
            merge_required.update(files_by_category.keys())
            
        # Log the fact that a merge is being forced.
        if merge_required:
            logger.info('status="forcing merge; stanzas have been modified" merge_classes="{}"'.format(merge_required))
        
        # Checkpoint the most recent data.
        self.set_checkpoint_data(PREVIOUS_STANZAS, files_by_category)
        
        return merge_required

    def run(self, stanzas, *args, **kwargs):

        logger.debug("Entering run method.")
        logger.debug("Input configuration: {0}".format(str(self._input_config)))
        logger.debug("Cleaned parameters: {0}".format(str(stanzas)))
        
        # If an alternative session key for testing was provided 
        # via command-line arguments, use it.
        try:
            if self._alt_session_key:
                self._input_config.session_key = self._alt_session_key
        except (AttributeError, NameError):
            # No alternate session key defined.
            pass
        
        # 0. Get the start time. This will be used for checkpoint data, to 
        # minimize the risk of the modular input failing to process updated 
        # files on subsequent runs (the timestamp of a file must be greater than
        # the last run time of this input to force a merge process.
        current_time = time.time()

        # 1. Check for existence of checkpoint directory.
        if not os.path.isdir(self._input_config.checkpoint_dir):
            os.mkdir(self._input_config.checkpoint_dir)

        # 2. Detect if we are the master host.
        # Note: Only the first stanza is checked; it is not technically an error
        # to specify the master_host setting in a non-default stanza for this 
        # input, but the value will be ignored.
        self._stanzas = stanzas
        exec_status, msg = should_execute(self._stanzas[0].get('master_host', ''))

        # 3. If the stanza definitions from another modular input are required,
        #    collect them now via REST. Save the original stanzas.
        self._orig_stanzas = copy.deepcopy(stanzas)
        self._stanzas = self.collect_stanzas(stanzas)

        if exec_status:

            # 4. Execute the modular input.
            logger.info('status="proceeding" msg={0})'.format(msg))

            # 5. Obtain the complete list of lookup table and/or checkpoint files.
            files_by_category = self.collect_files()
            
            # 6. Determine whether any files have been updated since the
            # last execution of the script.
            # Last script execution time is determined by a timestamp stored in
            # the checkpoint directory. If the timestamp is not available, the
            # merge process is forced.
            try:
                last_run = self.last_ran(self._input_config.checkpoint_dir, self.name)
                logger.info('status="retrieved last script run" value="%s"', time.ctime(last_run))
            except (IOError, ValueError):
                # There was no checkpoint file for the last run, or it could
                # not be read.
                # There are two possible cases:
                # a. This is the first run of this script.
                # b. The previous script aborted.
                # In either case, force execution of the merge, since we cannot 
                # assume that the previous merge process succeeded.
                # This is benign when it occurs on the first script run after startup.
                logger.error('status="last run checkpoint file not found"')
                last_run = 0

            # 7. Hand off to the run_threads method in the subclass for the merge
            # process. These threads should always exit. This method should
            # return only after the subsidiary threads have completed.
            success = self.run_threads(files_by_category, last_run)

            # 8. Update checkpoint.
            if success:
                logger.info('status="All threads completed successfully. Creating last run checkpoint." current_time="%s"', time.ctime(current_time))
                self.save_checkpoint(self._input_config.checkpoint_dir, self.name, current_time)
            else:
                logger.info('status="One or more threads did not complete. Skipping checkpoint creation." current_time="%s"', time.ctime(current_time))

        else:
            # 4b: Exit the script if the host is a pool member but not
            # designated as the master host for this input.
            logger.info('status="exiting, not master host" msg=%s', msg)
